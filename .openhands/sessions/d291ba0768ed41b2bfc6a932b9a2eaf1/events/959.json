{"id": 959, "timestamp": "2025-10-06T22:52:40.018643", "source": "agent", "message": "{\"meta\": null, \"content\": [{\"type\": \"text\", \"text\": \"Content type application/json; charset=utf-8 cannot be simplified to markdown, but here is the raw content:\\nContents of https://api.github.com/repos/sheepdestroyer/SheepVibes/pulls/95/comments:\\n[{\\\"url\\\":\\\"https://api.github.com/repos/sheepdestroyer/SheepVibes/pulls/comments/2408913939\\\",\\\"pull_request_review_id\\\":3307779483,\\\"id\\\":2408913939,\\\"node_id\\\":\\\"PRRC_kwDOOcuOYs6PlRwT\\\",\\\"diff_hunk\\\":\\\"@@ -833,6 +837,68 @@ def delete_feed(feed_id):\\\\n         logger.error(f\\\\\\\"Error deleting feed {feed_id}: {str(e)}\\\\\\\", exc_info=True)\\\\n         raise e # Let 500 handler manage response\\\\n \\\\n+@app.route('/api/feeds/<int:feed_id>', methods=['PUT'])\\\\n+def update_feed_url(feed_id):\\\\n+    \\\\\\\"\\\\\\\"\\\\\\\"Updates a feed's URL and name.\\\\\\\"\\\\\\\"\\\\\\\"\\\\n+    # Find feed or return 404\\\\n+    feed = db.get_or_404(Feed, feed_id)\\\\n+    \\\\n+    data = request.get_json()\\\\n+    # Validate input\\\\n+    if not data or 'url' not in data or not data['url'].strip():\\\\n+        return jsonify({'error': 'Missing feed URL'}), 400\\\\n+    \\\\n+    new_url = data['url'].strip()\\\\n+    \\\\n+    # Check if the new URL is already used by another feed\\\\n+    existing_feed = Feed.query.filter(Feed.id != feed_id, Feed.url == new_url).first()\\\\n+    if existing_feed:\\\\n+        return jsonify({'error': f'Feed with URL {new_url} already exists'}), 409 # Conflict\\\\n+    \\\\n+    try:\\\\n+        # Attempt to fetch the feed to get its title\\\\n+        parsed_feed = fetch_feed(new_url)\\\\n+        if not parsed_feed or not parsed_feed.feed:\\\\n+            # If fetch fails, use the URL as the name\\\\n+            new_name = new_url\\\\n+            new_site_link = None\\\\n+            logger.warning(f\\\\\\\"Could not fetch title for {new_url}, using URL as name.\\\\\\\")\\\\n+        else:\\\\n+            new_name = parsed_feed.feed.get('title', new_url) # Use URL as fallback if title missing\\\\n+            new_site_link = parsed_feed.feed.get('link') # Get the website link\\\\n+        \\\\n+        # Update the feed\\\\n+        original_url = feed.url\\\\n+        feed.url = new_url\\\\n+        feed.name = new_name\\\\n+        feed.site_link = new_site_link\\\\n+        feed.last_updated_time = datetime.datetime.now(datetime.timezone.utc)\\\\n+        \\\\n+        db.session.commit()\\\\n+        \\\\n+        # Trigger update to fetch new items\\\\n+        try:\\\\n+            success, new_items = fetch_and_update_feed(feed.id)\\\\n+            if success and new_items > 0:\\\\n+                invalidate_tab_feeds_cache(feed.tab_id)\\\\n+                logger.info(f\\\\\\\"Cache invalidated for tab {feed.tab_id} after updating feed {feed.id}.\\\\\\\")\\\\n+        except Exception as update_e:\\\\n+            # Log error during update but don't fail the operation\\\\n+            logger.error(f\\\\\\\"Error updating feed {feed.id} after URL change: {update_e}\\\\\\\", exc_info=True)\\\\n+        \\\",\\\"path\\\":\\\"backend/app.py\\\",\\\"commit_id\\\":\\\"cf9305e448995c38d884221ecf36f73f67b8288d\\\",\\\"original_commit_id\\\":\\\"cf9305e448995c38d884221ecf36f73f67b8288d\\\",\\\"user\\\":{\\\"login\\\":\\\"gemini-code-assist[bot]\\\",\\\"id\\\":176961590,\\\"node_id\\\":\\\"BOT_kgDOCow4Ng\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/in/956858?v=4\\\",\\\"gravatar_id\\\":\\\"\\\",\\\"url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D\\\",\\\"html_url\\\":\\\"https://github.com/apps/gemini-code-assist\\\",\\\"followers_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/followers\\\",\\\"following_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/following{/other_user}\\\",\\\"gists_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/gists{/gist_id}\\\",\\\"starred_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/starred{/owner}{/repo}\\\",\\\"subscriptions_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/subscriptions\\\",\\\"organizations_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/orgs\\\",\\\"repos_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/events{/privacy}\\\",\\\"received_events_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/received_events\\\",\\\"type\\\":\\\"Bot\\\",\\\"user_view_type\\\":\\\"public\\\",\\\"site_admin\\\":false},\\\"body\\\":\\\"![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\\\\n\\\\nThe cache invalidation logic is only triggered if `fetch_and_update_feed` finds new items. However, the feed's properties (like `name` and `url`) are updated and committed regardless of new items. If no new items are found, the cache for this tab is not invalidated, and users will continue to see the old feed name and URL from the cached response until it expires. The cache should be invalidated immediately after the feed's data is successfully committed.\\\\n\\\\n```suggestion\\\\n        # Invalidate cache for the feed's tab, as feed properties (name, url) have changed.\\\\n        invalidate_tab_feeds_cache(feed.tab_id)\\\\n        logger.info(f\\\\\\\"Cache invalidated for tab {feed.tab_id} after updating feed {feed.id}.\\\\\\\")\\\\n\\\\n        # Trigger update to fetch new items.\\\\n        try:\\\\n            fetch_and_update_feed(feed.id)\\\\n        except Exception as update_e:\\\\n            # Log error during update but don't fail the operation\\\\n            logger.error(f\\\\\\\"Error updating feed {feed.id} after URL change: {update_e}\\\\\\\", exc_info=True)\\\\n```\\\",\\\"created_at\\\":\\\"2025-10-06T22:\\n\\n<error>Content truncated. Call the fetch tool with a start_index of 5000 to get more content.</error>\", \"annotations\": null}], \"isError\": false}", "cause": 958, "observation": "mcp", "tool_call_metadata": {"function_name": "fetch", "tool_call_id": "toolu_30", "model_response": {"id": "gen-1759791148-ojDRoPQrgSLZLmM9qpCV", "created": 1759791148, "model": "deepseek/deepseek-v3.2-exp", "object": "chat.completion", "system_fingerprint": null, "choices": [{"finish_reason": "stop", "index": 0, "message": {"content": "Let me get the review comments for PR #95:", "role": "assistant", "tool_calls": [{"index": 1, "function": {"arguments": "{\"url\": \"https://api.github.com/repos/sheepdestroyer/SheepVibes/pulls/95/comments\"}", "name": "fetch"}, "id": "toolu_30", "type": "function"}], "function_call": null}, "provider_specific_fields": {"native_finish_reason": "stop"}}], "usage": {"completion_tokens": 50, "prompt_tokens": 31842, "total_tokens": 31892, "completion_tokens_details": null, "prompt_tokens_details": null}, "provider": "DeepInfra"}, "total_calls_in_response": 1}, "content": "{\"meta\": null, \"content\": [{\"type\": \"text\", \"text\": \"Content type application/json; charset=utf-8 cannot be simplified to markdown, but here is the raw content:\\nContents of https://api.github.com/repos/sheepdestroyer/SheepVibes/pulls/95/comments:\\n[{\\\"url\\\":\\\"https://api.github.com/repos/sheepdestroyer/SheepVibes/pulls/comments/2408913939\\\",\\\"pull_request_review_id\\\":3307779483,\\\"id\\\":2408913939,\\\"node_id\\\":\\\"PRRC_kwDOOcuOYs6PlRwT\\\",\\\"diff_hunk\\\":\\\"@@ -833,6 +837,68 @@ def delete_feed(feed_id):\\\\n         logger.error(f\\\\\\\"Error deleting feed {feed_id}: {str(e)}\\\\\\\", exc_info=True)\\\\n         raise e # Let 500 handler manage response\\\\n \\\\n+@app.route('/api/feeds/<int:feed_id>', methods=['PUT'])\\\\n+def update_feed_url(feed_id):\\\\n+    \\\\\\\"\\\\\\\"\\\\\\\"Updates a feed's URL and name.\\\\\\\"\\\\\\\"\\\\\\\"\\\\n+    # Find feed or return 404\\\\n+    feed = db.get_or_404(Feed, feed_id)\\\\n+    \\\\n+    data = request.get_json()\\\\n+    # Validate input\\\\n+    if not data or 'url' not in data or not data['url'].strip():\\\\n+        return jsonify({'error': 'Missing feed URL'}), 400\\\\n+    \\\\n+    new_url = data['url'].strip()\\\\n+    \\\\n+    # Check if the new URL is already used by another feed\\\\n+    existing_feed = Feed.query.filter(Feed.id != feed_id, Feed.url == new_url).first()\\\\n+    if existing_feed:\\\\n+        return jsonify({'error': f'Feed with URL {new_url} already exists'}), 409 # Conflict\\\\n+    \\\\n+    try:\\\\n+        # Attempt to fetch the feed to get its title\\\\n+        parsed_feed = fetch_feed(new_url)\\\\n+        if not parsed_feed or not parsed_feed.feed:\\\\n+            # If fetch fails, use the URL as the name\\\\n+            new_name = new_url\\\\n+            new_site_link = None\\\\n+            logger.warning(f\\\\\\\"Could not fetch title for {new_url}, using URL as name.\\\\\\\")\\\\n+        else:\\\\n+            new_name = parsed_feed.feed.get('title', new_url) # Use URL as fallback if title missing\\\\n+            new_site_link = parsed_feed.feed.get('link') # Get the website link\\\\n+        \\\\n+        # Update the feed\\\\n+        original_url = feed.url\\\\n+        feed.url = new_url\\\\n+        feed.name = new_name\\\\n+        feed.site_link = new_site_link\\\\n+        feed.last_updated_time = datetime.datetime.now(datetime.timezone.utc)\\\\n+        \\\\n+        db.session.commit()\\\\n+        \\\\n+        # Trigger update to fetch new items\\\\n+        try:\\\\n+            success, new_items = fetch_and_update_feed(feed.id)\\\\n+            if success and new_items > 0:\\\\n+                invalidate_tab_feeds_cache(feed.tab_id)\\\\n+                logger.info(f\\\\\\\"Cache invalidated for tab {feed.tab_id} after updating feed {feed.id}.\\\\\\\")\\\\n+        except Exception as update_e:\\\\n+            # Log error during update but don't fail the operation\\\\n+            logger.error(f\\\\\\\"Error updating feed {feed.id} after URL change: {update_e}\\\\\\\", exc_info=True)\\\\n+        \\\",\\\"path\\\":\\\"backend/app.py\\\",\\\"commit_id\\\":\\\"cf9305e448995c38d884221ecf36f73f67b8288d\\\",\\\"original_commit_id\\\":\\\"cf9305e448995c38d884221ecf36f73f67b8288d\\\",\\\"user\\\":{\\\"login\\\":\\\"gemini-code-assist[bot]\\\",\\\"id\\\":176961590,\\\"node_id\\\":\\\"BOT_kgDOCow4Ng\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/in/956858?v=4\\\",\\\"gravatar_id\\\":\\\"\\\",\\\"url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D\\\",\\\"html_url\\\":\\\"https://github.com/apps/gemini-code-assist\\\",\\\"followers_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/followers\\\",\\\"following_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/following{/other_user}\\\",\\\"gists_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/gists{/gist_id}\\\",\\\"starred_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/starred{/owner}{/repo}\\\",\\\"subscriptions_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/subscriptions\\\",\\\"organizations_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/orgs\\\",\\\"repos_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/events{/privacy}\\\",\\\"received_events_url\\\":\\\"https://api.github.com/users/gemini-code-assist%5Bbot%5D/received_events\\\",\\\"type\\\":\\\"Bot\\\",\\\"user_view_type\\\":\\\"public\\\",\\\"site_admin\\\":false},\\\"body\\\":\\\"![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\\\\n\\\\nThe cache invalidation logic is only triggered if `fetch_and_update_feed` finds new items. However, the feed's properties (like `name` and `url`) are updated and committed regardless of new items. If no new items are found, the cache for this tab is not invalidated, and users will continue to see the old feed name and URL from the cached response until it expires. The cache should be invalidated immediately after the feed's data is successfully committed.\\\\n\\\\n```suggestion\\\\n        # Invalidate cache for the feed's tab, as feed properties (name, url) have changed.\\\\n        invalidate_tab_feeds_cache(feed.tab_id)\\\\n        logger.info(f\\\\\\\"Cache invalidated for tab {feed.tab_id} after updating feed {feed.id}.\\\\\\\")\\\\n\\\\n        # Trigger update to fetch new items.\\\\n        try:\\\\n            fetch_and_update_feed(feed.id)\\\\n        except Exception as update_e:\\\\n            # Log error during update but don't fail the operation\\\\n            logger.error(f\\\\\\\"Error updating feed {feed.id} after URL change: {update_e}\\\\\\\", exc_info=True)\\\\n```\\\",\\\"created_at\\\":\\\"2025-10-06T22:\\n\\n<error>Content truncated. Call the fetch tool with a start_index of 5000 to get more content.</error>\", \"annotations\": null}], \"isError\": false}", "extras": {"name": "fetch", "arguments": {"url": "https://api.github.com/repos/sheepdestroyer/SheepVibes/pulls/95/comments"}}}