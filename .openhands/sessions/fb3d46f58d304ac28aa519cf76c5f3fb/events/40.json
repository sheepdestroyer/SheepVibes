{"id": 40, "timestamp": "2025-10-07T00:04:05.232499", "source": "agent", "message": "I read the file /workspace/SheepVibes/backend/feed_service.py.", "cause": 39, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "toolu_16", "model_response": {"id": "gen-1759795437-vhRDA1w3yD4HfzTMdByn", "created": 1759795437, "model": "deepseek/deepseek-v3.2-exp", "object": "chat.completion", "system_fingerprint": null, "choices": [{"finish_reason": "stop", "index": 0, "message": {"content": "Now let me look at the feed_service module to understand the performance issue:", "role": "assistant", "tool_calls": [{"index": 1, "function": {"arguments": "{\"command\": \"view\", \"path\": \"/workspace/SheepVibes/backend/feed_service.py\", \"security_risk\": \"LOW\"}", "name": "str_replace_editor"}, "id": "toolu_16", "type": "function"}], "function_call": null}, "provider_specific_fields": {"native_finish_reason": "stop"}}], "usage": {"completion_tokens": 68, "prompt_tokens": 31566, "total_tokens": 31634, "completion_tokens_details": null, "prompt_tokens_details": null}, "provider": "DeepInfra"}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /workspace/SheepVibes/backend/feed_service.py:\n     1\t# Import necessary libraries\n     2\timport feedparser\n     3\timport datetime # Import the full module\n     4\tfrom datetime import timezone # Specifically import timezone\n     5\timport logging # Standard logging\n     6\timport ssl # Added for specific SSL error catching\n     7\tfrom dateutil import parser as date_parser # Use dateutil for robust date parsing\n     8\tfrom sqlalchemy.exc import IntegrityError\n     9\t\n    10\t# Import database models from the new models.py\n    11\tfrom .models import db, Feed, FeedItem\n    12\t\n    13\t# Set up logger for this module\n    14\tlogger = logging.getLogger(__name__)\n    15\t\n    16\t# --- Helper Functions ---\n    17\t\n    18\tdef parse_published_time(entry):\n    19\t    \"\"\"Attempts to parse the published time from a feed entry.\n    20\t\n    21\t    Args:\n    22\t        entry: A feedparser entry object.\n    23\t\n    24\t    Returns:\n    25\t        A datetime object representing the published time (UTC aware).\n    26\t        Falls back to current UTC time if parsing fails.\n    27\t    \"\"\"\n    28\t    parsed_dt = None\n    29\t    if hasattr(entry, 'published_parsed') and entry.published_parsed:\n    30\t        try:\n    31\t            parsed_dt = datetime.datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)\n    32\t        except (TypeError, ValueError) as e:\n    33\t            logger.debug(f\"Failed to parse 'published_parsed' for entry {entry.get('link', '[no link]')}: {e}\")\n    34\t            parsed_dt = None\n    35\t    \n    36\t    if parsed_dt is None:\n    37\t        # Try common date fields using dateutil.parser for more flexibility\n    38\t        # Added 'dc:date' as observed in some feeds\n    39\t        date_fields = ['published', 'updated', 'created', 'dc:date']\n    40\t        for field in date_fields:\n    41\t            if hasattr(entry, field):\n    42\t                field_value = getattr(entry, field)\n    43\t                if field_value:\n    44\t                    try:\n    45\t                        parsed_dt = date_parser.parse(field_value)\n    46\t                        if parsed_dt:\n    47\t                            break\n    48\t                    except (ValueError, TypeError, OverflowError) as e:\n    49\t                        logger.debug(f\"Specific parsing error for date field '{field}' for entry {entry.get('link', '[no link]')} ({type(e).__name__}): {e}\")\n    50\t                        continue\n    51\t                    except Exception as e:\n    52\t                        logger.warning(f\"Generic parsing error for date field '{field}' for entry {entry.get('link', '[no link]')} ({type(e).__name__}): {e}\")\n    53\t                        continue\n    54\t\n    55\t    if parsed_dt:\n    56\t        # Ensure the datetime is UTC timezone-aware\n    57\t        if parsed_dt.tzinfo is None or parsed_dt.tzinfo.utcoffset(parsed_dt) is None:\n    58\t            parsed_dt = parsed_dt.replace(tzinfo=timezone.utc)\n    59\t        else:\n    60\t            parsed_dt = parsed_dt.astimezone(timezone.utc)\n    61\t        return parsed_dt\n    62\t                \n    63\t    logger.warning(f\"Could not parse published time for entry: {entry.get('link', '[no link]')}. Using current time as fallback.\")\n    64\t    return datetime.datetime.now(timezone.utc)\n    65\t\n    66\t# --- Core Feed Processing Functions ---\n    67\t\n    68\tdef fetch_feed(feed_url):\n    69\t    \"\"\"Fetches and parses a feed using feedparser.\n    70\t\n    71\t    Args:\n    72\t        feed_url: The URL of the RSS/Atom feed.\n    73\t\n    74\t    Returns:\n    75\t        A feedparser dictionary object, or None if fetching/parsing fails.\n    76\t    \"\"\"\n    77\t    logger.info(f\"Fetching feed: {feed_url}\")\n    78\t    try:\n    79\t        parsed_feed = feedparser.parse(feed_url)\n    80\t        \n    81\t        if parsed_feed.bozo:\n    82\t            bozo_exc = parsed_feed.get('bozo_exception', Exception('Unknown parsing error (bozo=1)'))\n    83\t            error_message = str(bozo_exc)\n    84\t\n    85\t            if \"SSL\" in error_message.upper() or \\\n    86\t               \"CERTIFICATE\" in error_message.upper() or \\\n    87\t               isinstance(bozo_exc, ssl.SSLError):\n    88\t                logger.error(\n    89\t                    f\"Failed to fetch feed {feed_url} due to SSL/Certificate error (feedparser bozo): {error_message}\",\n    90\t                    exc_info=False\n    91\t                )\n    92\t            else:\n    93\t                logger.warning(f\"Feed is ill-formed: {feed_url} - Error: {error_message}\")\n    94\t\n    95\t        if not parsed_feed.entries and not parsed_feed.bozo:\n    96\t             logger.warning(f\"No entries found in feed (and not a bozo feed): {feed_url}\")\n    97\t\n    98\t        if not parsed_feed.bozo:\n    99\t            logger.info(f\"Successfully fetched feed: {feed_url}\")\n   100\t        return parsed_feed\n   101\t\n   102\t    except ssl.SSLError as ssl_e:\n   103\t        logger.error(f\"Direct SSL Error during fetch attempt for {feed_url}: {ssl_e}\", exc_info=True)\n   104\t        return None\n   105\t    except Exception as e:\n   106\t        logger.error(f\"Generic error fetching or parsing feed {feed_url}: {e}\", exc_info=True)\n   107\t        return None\n   108\t\n   109\tdef process_feed_entries(feed_db_obj, parsed_feed):\n   110\t    \"\"\"Processes entries from a parsed feed and adds new items to the database.\n   111\t\n   112\t    Args:\n   113\t        feed_db_obj: The Feed database object (SQLAlchemy model instance).\n   114\t        parsed_feed: The dictionary object returned by feedparser.parse().\n   115\t\n   116\t    Returns:\n   117\t        The number of new items added to the database for this feed.\n   118\t    \"\"\"\n   119\t    if not parsed_feed:\n   120\t        logger.error(f\"process_feed_entries called with a null parsed_feed for feed ID {feed_db_obj.id if feed_db_obj else 'Unknown'}\")\n   121\t        return 0\n   122\t\n   123\t    # These sets track items *within the current batch being processed*\n   124\t    batch_processed_guids = set()\n   125\t    batch_processed_links = set()\n   126\t\n   127\t    # Get existing GUIDs and links *for this specific feed* from the DB\n   128\t    existing_feed_items_query = db.session.query(FeedItem.guid, FeedItem.link).filter_by(feed_id=feed_db_obj.id)\n   129\t    # Ensure we only add non-None GUIDs to the set\n   130\t    existing_feed_guids = {item.guid for item in existing_feed_items_query if item.guid}\n   131\t    # Ensure we only add non-None links to the set (though link is NOT NULL in DB)\n   132\t    existing_feed_links = {item.link for item in existing_feed_items_query if item.link}\n   133\t\n   134\t\n   135\t    new_title = parsed_feed.feed.get('title')\n   136\t    if new_title and new_title.strip() and new_title != feed_db_obj.name:\n   137\t        logger.info(f\"Updating feed title for '{feed_db_obj.name}' to '{new_title}'\")\n   138\t        feed_db_obj.name = new_title\n   139\t\n   140\t    # Update site_link if available and different\n   141\t    new_site_link = parsed_feed.feed.get('link') # This is typically the website link\n   142\t    if new_site_link and new_site_link.strip() and new_site_link != feed_db_obj.site_link:\n   143\t        logger.info(f\"Updating feed site_link for '{feed_db_obj.name}' from '{feed_db_obj.site_link}' to '{new_site_link}'\")\n   144\t        feed_db_obj.site_link = new_site_link\n   145\t    elif not feed_db_obj.site_link and new_site_link and new_site_link.strip(): # If current is null, set it\n   146\t        logger.info(f\"Setting feed site_link for '{feed_db_obj.name}' to '{new_site_link}'\")\n   147\t        feed_db_obj.site_link = new_site_link\n   148\t\n   149\t\n   150\t    logger.info(f\"Processing {len(parsed_feed.entries)} entries for feed: {feed_db_obj.name} (ID: {feed_db_obj.id})\")\n   151\t\n   152\t    items_to_add = []\n   153\t\n   154\t    for entry in parsed_feed.entries:\n   155\t        entry_title = entry.get('title', '[No Title]')\n   156\t        entry_link = entry.get('link')\n   157\t        feedparser_id = entry.get('id') # This is what feedparser provides as 'id'\n   158\t\n   159\t        if not entry_link: # Link is essential\n   160\t            logger.warning(f\"Skipping entry titled '{entry_title[:100]}' for feed '{feed_db_obj.name}' due to missing link. (feedparser ID: {feedparser_id if feedparser_id else 'N/A'})\")\n   161\t            continue\n   162\t\n   163\t        # Determine the GUID to be stored in the database (db_guid).\n   164\t        # The GUID is essential for uniquely identifying feed items over time.\n   165\t        # While feedparser provides an 'id' field, it can be unreliable or non-unique in some feeds.\n   166\t        # The item's link, however, is almost always unique.\n   167\t        #\n   168\t        # Previously, the logic tried to create a \"true\" GUID only if the feedparser 'id' was\n   169\t        # different from the link, otherwise leaving it as None. This led to issues when\n   170\t        # a feed used the same non-link 'id' for multiple, different items, causing a UNIQUE\n   171\t        # constraint violation on (feed_id, guid).\n   172\t        #\n   173\t        # The new strategy is to always use the entry's link as the primary GUID. This ensures\n   174\t        # that every item has a reliable, unique identifier, preventing database errors and\n   175\t        # ensuring that updates are processed correctly.\n   176\t        db_guid = entry_link\n   177\t\n   178\t        # --- Deduplication Logic ---\n   179\t        # 1. Check against items already in the DB *for this specific feed*\n   180\t        if db_guid and db_guid in existing_feed_guids:\n   181\t            # logger.debug(f\"Item with db_guid '{db_guid}' already exists in DB for feed '{feed_db_obj.name}'. Skipping.\")\n   182\t            continue\n   183\t        # Check link for this feed, critical for items that will have db_guid=None or if link is the primary identifier\n   184\t        if entry_link in existing_feed_links:\n   185\t            # logger.debug(f\"Item with link '{entry_link}' already exists in DB for feed '{feed_db_obj.name}'. Skipping. (db_guid was: {db_guid})\")\n   186\t            continue\n   187\t\n   188\t        # 2. Check against items already processed *in this current batch*\n   189\t        is_batch_duplicate = False\n   190\t        if db_guid: # If this item has a \"true\" GUID\n   191\t            if db_guid in batch_processed_guids:\n   192\t                logger.warning(f\"Skipping item (true GUID: {db_guid}, link: {entry_link}) for feed '{feed_db_obj.name}', duplicate true GUID in current fetch batch.\")\n   193\t                is_batch_duplicate = True\n   194\t        else: # No \"true\" GUID (db_guid is None), so batch uniqueness relies on the link\n   195\t            if entry_link in batch_processed_links:\n   196\t                logger.warning(f\"Skipping item (link: {entry_link}) for feed '{feed_db_obj.name}', it has no true GUID and its link is a duplicate in current fetch batch.\")\n   197\t                is_batch_duplicate = True\n   198\t\n   199\t        if is_batch_duplicate:\n   200\t            continue\n   201\t\n   202\t        # If we reach here, the item is considered new.\n   203\t        # Add its identifiers to the batch processed sets for subsequent checks within this batch.\n   204\t        if db_guid:\n   205\t            batch_processed_guids.add(db_guid)\n   206\t        # Always add the link to batch_processed_links. For items with true GUIDs, this helps\n   207\t        # catch subsequent items in the same batch that *lack* a true GUID but share the same link.\n   208\t        # For items without true GUIDs, this is their primary batch duplicate check.\n   209\t        batch_processed_links.add(entry_link)\n   210\t\n   211\t        published_time = parse_published_time(entry) # Already falls back to now()\n   212\t\n   213\t        new_item = FeedItem(\n   214\t            feed_id=feed_db_obj.id,\n   215\t            title=entry_title,\n   216\t            link=entry_link,\n   217\t            published_time=published_time,\n   218\t            guid=db_guid # This will be None if feedparser_id was missing or same as link\n   219\t        )\n   220\t        items_to_add.append(new_item)\n   221\t\n   222\t    if not items_to_add:\n   223\t        feed_db_obj.last_updated_time = datetime.datetime.now(timezone.utc)\n   224\t        try:\n   225\t            db.session.commit() # Commit to save updated last_updated_time\n   226\t        except Exception as e: # Catch potential errors during this commit\n   227\t            db.session.rollback()\n   228\t            logger.error(f\"Error committing feed update (no new items) for {feed_db_obj.name}: {e}\", exc_info=True)\n   229\t        return 0\n   230\t\n   231\t    committed_items_count = 0\n   232\t    try:\n   233\t        db.session.add_all(items_to_add)\n   234\t        feed_db_obj.last_updated_time = datetime.datetime.now(timezone.utc) # Set time before trying to commit\n   235\t        db.session.commit()\n   236\t        committed_items_count = len(items_to_add)\n   237\t        logger.info(f\"Successfully batch-added {committed_items_count} new items for feed: {feed_db_obj.name}\")\n   238\t    except IntegrityError as e:\n   239\t        db.session.rollback() # Rollback the failed batch\n   240\t        logger.warning(f\"Batch insert failed for feed '{feed_db_obj.name}' due to IntegrityError: {e}. Attempting individual inserts.\")\n   241\t\n   242\t        # Ensure last_updated_time is set even if all individual inserts fail,\n   243\t        # as the feed itself was successfully fetched and processed up to this point.\n   244\t        # This needs to be part of a new transaction if the previous one was rolled back.\n   245\t        try:\n   246\t            feed_db_obj.last_updated_time = datetime.datetime.now(timezone.utc)\n   247\t            db.session.add(feed_db_obj) # Re-add if it became detached after rollback\n   248\t            db.session.commit()\n   249\t        except Exception as ts_e:\n   250\t            db.session.rollback()\n   251\t            logger.error(f\"Error updating last_updated_time for feed '{feed_db_obj.name}' after batch insert failure: {ts_e}\", exc_info=True)\n   252\t\n   253\t        for item_to_add in items_to_add:\n   254\t            try:\n   255\t                # Each item needs to be added to a fresh session or re-added if the session was rolled back.\n   256\t                # If items were expunged by rollback, they might need to be re-created or merged.\n   257\t                # Simplest is to re-add to current session if it's still active for individual commits.\n   258\t                db.session.add(item_to_add) # Re-add the item to the session\n   259\t                db.session.commit()\n   260\t                committed_items_count += 1\n   261\t                logger.debug(f\"Individually added item: {item_to_add.title[:50]} for feed '{feed_db_obj.name}'\")\n   262\t            except IntegrityError as ie_individual:\n   263\t                db.session.rollback() # Rollback this specific item's commit\n   264\t                logger.error(f\"Failed to individually add item '{item_to_add.title[:100]}' (link: {item_to_add.link}, guid: {item_to_add.guid}) for feed '{feed_db_obj.name}': {ie_individual}\", exc_info=False) # Log less verbosely for individual fails\n   265\t            except Exception as e_individual:\n   266\t                db.session.rollback()\n   267\t                logger.error(f\"Generic error individually adding item '{item_to_add.title[:100]}' for feed '{feed_db_obj.name}': {e_individual}\", exc_info=True)\n   268\t\n   269\t        if committed_items_count > 0:\n   270\t            logger.info(f\"Successfully added {committed_items_count} items individually for feed: {feed_db_obj.name} after batch failure.\")\n   271\t        else:\n   272\t            logger.info(f\"No items could be added individually for feed: {feed_db_obj.name} after batch failure.\")\n   273\t\n   274\t    except Exception as e:\n   275\t        db.session.rollback()\n   276\t        logger.error(f\"Generic error committing new items for feed {feed_db_obj.name}: {e}\", exc_info=True)\n   277\t        return 0 # Return 0 as no items were successfully committed in this case\n   278\t\n   279\t    return committed_items_count\n   280\t\n   281\tdef fetch_and_update_feed(feed_id):\n   282\t    \"\"\"Fetches a single feed by ID, processes its entries, and updates the database.\n   283\t\n   284\t    Args:\n   285\t        feed_id: The database ID of the Feed to update.\n   286\t\n   287\t    Returns:\n   288\t        A tuple (success, new_items_count):\n   289\t        - success (bool): True if the feed was fetched and processed successfully (even if 0 new items), False otherwise.\n   290\t        - new_items_count (int): The number of new items added.\n   291\t    \"\"\"\n   292\t    feed = db.session.get(Feed, feed_id)\n   293\t    if not feed:\n   294\t        logger.error(f\"Feed with ID {feed_id} not found for update.\")\n   295\t        return False, 0\n   296\t\n   297\t    parsed_feed = fetch_feed(feed.url)\n   298\t    if not parsed_feed:\n   299\t        logger.error(f\"Fetching content for feed '{feed.name}' (ID: {feed_id}) failed because fetch_feed returned None.\")\n   300\t        # Optionally update last_updated_time to now with a failure status\n   301\t        # feed.last_updated_time = datetime.datetime.now(timezone.utc)\n   302\t        # db.session.commit() # Be careful with commits in error paths\n   303\t        return False, 0\n   304\t\n   305\t    # Handle cases where feed is fetched but has no entries (common for new or empty feeds)\n   306\t    if not parsed_feed.entries:\n   307\t        logger.info(f\"Feed '{feed.name}' (ID: {feed_id}) fetched successfully but contained no entries.\")\n   308\t        feed.last_updated_time = datetime.datetime.now(timezone.utc)\n   309\t        try:\n   310\t            db.session.commit()\n   311\t        except Exception as e:\n   312\t            db.session.rollback()\n   313\t            logger.error(f\"Error committing feed update (no entries) for {feed.name}: {e}\", exc_info=True)\n   314\t            # Still, the fetch itself might be considered a \"success\" in terms of reachability\n   315\t        return True, 0\n   316\t\n   317\t    try:\n   318\t        new_items = process_feed_entries(feed, parsed_feed)\n   319\t        # process_feed_entries handles its own logging and commits for items and last_updated_time.\n   320\t        # Success here means process_feed_entries completed without raising an exception to this level.\n   321\t        return True, new_items\n   322\t    except E<response clipped><NOTE>Due to the max output limit, only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>\n", "extras": {"path": "/workspace/SheepVibes/backend/feed_service.py", "impl_source": "oh_aci"}}